# ğŸ¤– White Lightning ONNX Model Testing Framework

A comprehensive cross-language testing framework for ONNX models with support for **Binary Classification** (sentiment analysis) and **Multiclass Classification** (topic classification) across 7 programming languages.

## ğŸš€ Available Workflows

### 1. **Individual Model Testing** (`onnx-model-tests.yml`)
Run tests for **specific models and languages** with custom text input:
- âœ… **Flexible**: Choose any combination of model type + language
- âœ… **Custom Input**: Test with your own text
- âœ… **Detailed Output**: Comprehensive performance analysis
- âœ… **Manual Dispatch**: Run on-demand with custom parameters

### 2. **Comprehensive Testing** (`comprehensive-onnx-tests.yml`) 
Run **all 14 combinations** automatically with standardized inputs:
- âœ… **Complete Coverage**: Tests 2 models Ã— 7 languages = 14 combinations
- âœ… **Standardized**: Uses consistent test inputs for comparison
- âœ… **Automated**: Runs on push/PR + manual dispatch available
- âœ… **Performance Comparison**: Easy to compare across languages

## ğŸ“Š What Information You'll See

Every test run provides standardized output in this format:

```
ğŸ¤– ONNX [BINARY/MULTICLASS] CLASSIFIER - [LANGUAGE] IMPLEMENTATION
===============================================================
ğŸ”„ Processing: [Test Text]

ğŸ’» SYSTEM INFORMATION:
   Platform: Linux/macOS/Windows
   Processor: CPU Name
   CPU Cores: X physical, Y logical
   Total Memory: N GB
   Runtime: Language Implementation Version

ğŸ“Š [SENTIMENT/TOPIC] ANALYSIS RESULTS:
   ğŸ† Predicted [Sentiment/Topic]: POSITIVE/NEGATIVE or POLITICS/TECH/etc
   ğŸ“ˆ Confidence: XX.XX% (0.XXXX)
   ğŸ“ Input Text: "Your test text here"

ğŸ“ˆ PERFORMANCE SUMMARY:
   Total Processing Time: Tms
   â”£â” Preprocessing: Xms (X%)
   â”£â” Model Inference: Yms (Y%)
   â”—â” Postprocessing: Zms (Z%)

ğŸš€ THROUGHPUT:
   Texts per second: TPS

ğŸ’¾ RESOURCE USAGE:
   Memory Start: MB
   Memory End: MB
   Memory Delta: +MB
   CPU Usage: avg% avg, peak% peak (N samples)

ğŸ¯ PERFORMANCE RATING: ğŸš€ EXCELLENT / âœ… GOOD / âš ï¸ ACCEPTABLE / ğŸŒ SLOW
   (Tms total - Target: <100ms)
```

## ğŸ¯ Standard Test Inputs

- **Binary Classifier**: `"It was very bad purchase"` (sentiment analysis)
- **Multiclass Classifier**: `"President signs new legislation on healthcare reform"` (topic classification)

## ğŸ› ï¸ Supported Languages

| Language | Binary Classifier | Multiclass Classifier | Status |
|----------|-------------------|----------------------|---------|
| **Python** | âœ… | âœ… | Full Support |
| **Java** | âœ… | âœ… | Full Support |
| **C++** | âœ… | âœ… | Full Support |
| **C** | âœ… | âœ… | Full Support |
| **Node.js** | âœ… | âœ… | Full Support |
| **Rust** | âœ… | âœ… | Full Support |
| **Dart/Flutter** | âœ… | âœ… | Full Support |
| **Swift** | âš ï¸ | âš ï¸ | Coming Soon |

## ğŸ”§ How to Use This Repository

### 1. **Clone the Repository**
```bash
git clone https://github.com/your-org/whitelightning-test.git
cd whitelightning-test
```

### 2. **Add Your Models**
Place your ONNX models in the appropriate directories:

```
tests/
â”œâ”€â”€ binary_classifier/
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”œâ”€â”€ model.onnx          # Your binary classification model
â”‚   â”‚   â”œâ”€â”€ vocab.json          # Vocabulary file
â”‚   â”‚   â””â”€â”€ scaler.json         # Preprocessing scaler
â”‚   â”œâ”€â”€ java/
â”‚   â”œâ”€â”€ cpp/
â”‚   â””â”€â”€ [other languages]/
â””â”€â”€ multiclass_classifier/
    â”œâ”€â”€ python/
    â”‚   â”œâ”€â”€ model.onnx          # Your multiclass model
    â”‚   â”œâ”€â”€ vocab.json          # Vocabulary file
    â”‚   â””â”€â”€ scaler.json         # Preprocessing scaler
    â””â”€â”€ [other languages]/
```

### 3. **Read Language-Specific Documentation**
Each language implementation has its own README with specific setup instructions:

- ğŸ“ `tests/binary_classifier/python/README.md` - Python setup
- ğŸ“ `tests/binary_classifier/java/README.md` - Java setup  
- ğŸ“ `tests/binary_classifier/cpp/README.md` - C++ setup
- ğŸ“ `tests/binary_classifier/c/README.md` - C setup
- ğŸ“ `tests/binary_classifier/nodejs/README.md` - Node.js setup
- ğŸ“ `tests/binary_classifier/rust/README.md` - Rust setup
- ğŸ“ `tests/binary_classifier/dart/README.md` - Dart/Flutter setup

*The same structure exists for `multiclass_classifier/`*

### 4. **Test Locally**
Navigate to any language directory and run the tests:

```bash
# Example: Test Python implementation
cd tests/binary_classifier/python
python test_onnx_model.py "Your custom text here"

# Example: Test Rust implementation  
cd tests/binary_classifier/rust
cargo run --release -- "Your custom text here"

# Example: Test Node.js implementation
cd tests/binary_classifier/nodejs
node test_onnx_model.js "Your custom text here"
```

### 5. **Run GitHub Actions Workflows**

#### **Individual Testing** (Custom Input)
1. Go to **Actions** â†’ **ONNX Model Tests**
2. Click **Run workflow**
3. Select:
   - **Model Type**: `binary_classifier` or `multiclass_classifier`
   - **Language**: `python`, `java`, `cpp`, `c`, `nodejs`, `rust`, or `dart`
   - **Custom Text**: Your test input (optional)

#### **Comprehensive Testing** (All Languages)
1. Go to **Actions** â†’ **Comprehensive ONNX Tests**
2. Click **Run workflow** (uses standard test inputs)
3. View results for all 14 language-model combinations

## ğŸ“‹ Requirements

### Model Files
Each implementation expects these files:
- **`model.onnx`** - Your trained ONNX model
- **`vocab.json`** - Vocabulary mapping for text preprocessing
- **`scaler.json`** - Feature scaling parameters

### Dependencies
Each language has its own dependencies listed in:
- Python: `requirements.txt`
- Java: `pom.xml`
- C++: `CMakeLists.txt` or `Makefile`
- C: `Makefile`
- Node.js: `package.json`
- Rust: `Cargo.toml`
- Dart: `pubspec.yaml`

## ğŸ¯ Performance Benchmarking

The framework provides detailed performance metrics:

- â±ï¸ **Timing Analysis**: Preprocessing, inference, and postprocessing times
- ğŸ’¾ **Memory Usage**: Memory consumption tracking
- ğŸ–¥ï¸ **CPU Monitoring**: Average and peak CPU usage
- ğŸš€ **Throughput**: Texts processed per second
- ğŸ“Š **Performance Rating**: Automatic classification based on speed

## ğŸ¤ Contributing

1. **Add New Languages**: Create implementation in `tests/[model_type]/[language]/`
2. **Improve Performance**: Optimize existing implementations
3. **Add Features**: Enhance testing capabilities
4. **Update Documentation**: Keep language-specific READMEs current

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ› Issues & Support

- **GitHub Issues**: Report bugs or request features
- **Discussions**: Ask questions or share improvements
- **Wiki**: Detailed documentation and guides

---

*Happy testing! ğŸš€ Compare ONNX model performance across languages and find the best implementation for your use case.*
